{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGm/zBchVJeBlq6QwM8k3E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXyIXVG0lPsn","executionInfo":{"status":"ok","timestamp":1712644800766,"user_tz":-480,"elapsed":460,"user":{"displayName":"Linhao Wu","userId":"04058453859257591408"}},"outputId":"83bb970e-1d8d-49ff-d700-bd60b5ea33e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\tzip warning: name not matched: testreturn.ipynb\n","\n","zip error: Nothing to do! (try: zip -r testreturn.zip . -i testreturn.ipynb)\n"]}],"source":["from fastapi import FastAPI, Request\n","from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n","import uvicorn\n","import json\n","import datetime\n","import torch\n","\n","# 设置设备参数\n","DEVICE = \"cuda\"  # 使用CUDA\n","DEVICE_ID = \"0\"  # CUDA设备ID，如果未设置则为空\n","CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE  # 组合CUDA设备信息\n","\n","# 清理GPU内存函数\n","def torch_gc():\n","    if torch.cuda.is_available():  # 检查是否可用CUDA\n","        with torch.cuda.device(CUDA_DEVICE):  # 指定CUDA设备\n","            torch.cuda.empty_cache()  # 清空CUDA缓存\n","            torch.cuda.ipc_collect()  # 收集CUDA内存碎片\n","\n","# 创建FastAPI应用\n","app = FastAPI()\n","\n","# 处理POST请求的端点\n","@app.post(\"/\")\n","async def create_item(request: Request):\n","    global model, tokenizer  # 声明全局变量以便在函数内部使用模型和分词器\n","    json_post_raw = await request.json()  # 获取POST请求的JSON数据\n","    json_post = json.dumps(json_post_raw)  # 将JSON数据转换为字符串\n","    json_post_list = json.loads(json_post)  # 将字符串转换为Python对象\n","    prompt = json_post_list.get('prompt')  # 获取请求中的提示\n","    history = json_post_list.get('history')  # 获取请求中的历史记录\n","    max_length = json_post_list.get('max_length')  # 获取请求中的最大长度\n","    top_p = json_post_list.get('top_p')  # 获取请求中的top_p参数\n","    temperature = json_post_list.get('temperature')  # 获取请求中的温度参数\n","    # 调用模型进行对话生成\n","    response, history = model.chat(\n","        tokenizer,\n","        prompt,\n","        history=history,\n","        max_length=max_length if max_length else 2048,  # 如果未提供最大长度，默认使用2048\n","        top_p=top_p if top_p else 0.7,  # 如果未提供top_p参数，默认使用0.7\n","        temperature=temperature if temperature else 0.95  # 如果未提供温度参数，默认使用0.95\n","    )\n","    now = datetime.datetime.now()  # 获取当前时间\n","    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")  # 格式化时间为字符串\n","    # 构建响应JSON\n","    answer = {\n","        \"response\": response,\n","        \"history\": history,\n","        \"status\": 200,\n","        \"time\": time\n","    }\n","    # 构建日志信息\n","    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n","    print(log)  # 打印日志\n","    torch_gc()  # 执行GPU内存清理\n","    return answer  # 返回响应\n","\n","# 主函数入口\n","if __name__ == '__main__':\n","    # 加载预训练的分词器和模型\n","    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n","    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n","    model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定\n","    model.eval()  # 设置模型为评估模式\n","    # 启动FastAPI应用，用6006端口映射到本地，从而在本地使用api\n","    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # 在指定端口和主机上启动应用"]}]}